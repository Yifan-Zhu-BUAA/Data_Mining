\documentclass[12pt,a4paper]{article}
\usepackage[UTF8]{ctex}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}

% 代码样式设置
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    backgroundcolor=\color{gray!10},
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    frame=single,
    rulecolor=\color{black},
    tabsize=2,
    captionpos=b,
    breaklines=true,
    breakatwhitespace=false,
    escapeinside={\%*}{*)}
}

\title{图像聚类任务报告}
\author{数据挖掘课程作业}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{问题形式化描述}

\subsection{问题定义}

给定一个包含 $N$ 张图像的图像数据集 $\mathcal{D} = \{I_1, I_2, \ldots, I_N\}$，其中每张图像 $I_i$ 属于某个未知的类别。图像聚类的目标是将这些图像自动分组为 $K$ 个簇（clusters），使得：

\begin{itemize}
    \item 同一簇内的图像在某种特征空间中相似度较高
    \item 不同簇之间的图像相似度较低
    \item 聚类结果尽可能接近真实的类别分布
\end{itemize}

\subsection{数学形式化}

设图像数据集为 $\mathcal{D} = \{I_1, I_2, \ldots, I_N\}$，其中 $N = 600$。通过特征提取函数 $f: \mathcal{I} \rightarrow \mathbb{R}^d$，将每张图像 $I_i$ 映射到 $d$ 维特征空间：

\begin{equation}
\mathbf{x}_i = f(I_i) \in \mathbb{R}^d
\end{equation}

其中 $\mathbf{x}_i$ 是图像 $I_i$ 的特征向量。

聚类算法 $\mathcal{C}$ 将特征矩阵 $\mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N]^T \in \mathbb{R}^{N \times d}$ 映射到标签向量 $\mathbf{y} \in \{0, 1, \ldots, K-1\}^N$：

\begin{equation}
\mathbf{y} = \mathcal{C}(\mathbf{X}, K)
\end{equation}

其中 $K$ 是聚类数量（在本任务中 $K = 6$）。

\subsection{数据集描述}

本任务使用的数据集包含：
\begin{itemize}
    \item \textbf{图像数量}：600张
    \item \textbf{类别数量}：6个类别
    \item \textbf{类别分布}：每个类别100张图像
    \item \textbf{图像格式}：PNG格式
\end{itemize}

\subsection{类别详细说明}

数据集包含以下6个类别的图像，每个类别100张图像：

\begin{enumerate}
    \item \textbf{cable（电缆）}：电缆横截面图像，显示多根绝缘电线（通常包含黄绿色、蓝色、棕色等不同颜色的绝缘层）被包裹在白色外护套中，可以看到铜线芯的金属光泽。
    
    \item \textbf{tile（瓷砖）}：瓷砖纹理图像，呈现花岗岩或水磨石般的颗粒状纹理，以浅灰色或灰白色为背景，散布着大小不规则的深灰色或黑色斑点。
    
    \item \textbf{bottle（瓶子）}：瓶子顶部俯视图，显示瓶口和瓶颈的圆形开口，外边缘呈金属色或深棕色（可能是螺纹盖或厚玻璃边缘），内部为深色或黑色，带有轻微反光。
    
    \item \textbf{pill（药丸）}：白色椭圆形药丸图像，表面有压印文字（如"FF"），药丸表面散布着小的不规则红褐色斑点，背景为深色或黑色。
    
    \item \textbf{leather（皮革）}：皮革纹理图像，呈现中等到深红棕色的均匀纹理表面，具有皮革特有的细密纹理和细微皱纹，展现天然皮革的质感。
    
    \item \textbf{transistor（晶体管）}：电子元件图像，显示黑色三脚晶体管安装在浅棕色穿孔板或面包板上，晶体管主体为黑色，三个金属引脚呈银白色，已焊接或插入电路板孔中。
\end{enumerate}

\section{图像特征处理}

\subsection{特征提取方法}

图像聚类任务的关键在于如何将高维的图像数据转换为低维的特征表示。本任务实现了多种特征提取方法：

\subsubsection{深度学习方法：ResNet50}

使用预训练的ResNet50模型提取深度特征。ResNet50是在ImageNet数据集上预训练的卷积神经网络，能够提取图像的深层语义特征。

\textbf{优点}：
\begin{itemize}
    \item 能够捕获图像的语义信息
    \item 对图像的平移、旋转等变换具有较好的鲁棒性
    \item 特征维度固定（2048维），便于后续处理
\end{itemize}

\textbf{实现过程}：
\begin{enumerate}
    \item 将图像调整为 $224 \times 224$ 像素
    \item 进行标准化处理（使用ImageNet的均值和标准差）
    \item 通过ResNet50的前向传播提取特征
    \item 移除最后的全连接层，使用全局平均池化后的特征（2048维）
\end{enumerate}

\subsubsection{传统方法：HOG特征}

方向梯度直方图（Histogram of Oriented Gradients, HOG）是一种基于梯度的特征描述符。

\textbf{优点}：
\begin{itemize}
    \item 对光照变化不敏感
    \item 能够描述图像的局部形状信息
    \item 计算效率较高
\end{itemize}

\textbf{参数设置}：
\begin{itemize}
    \item 方向数：9
    \item 像素单元大小：$8 \times 8$
    \item 块大小：$2 \times 2$
\end{itemize}

\subsubsection{颜色特征：颜色直方图}

提取RGB三个通道的颜色直方图作为特征。

\textbf{优点}：
\begin{itemize}
    \item 计算简单快速
    \item 对图像的几何变换不敏感
    \item 能够描述图像的整体颜色分布
\end{itemize}

\textbf{实现}：对每个颜色通道计算256个bin的直方图，共768维特征。

\subsubsection{组合特征}

将多种特征方法组合使用，例如：ResNet特征 + HOG特征 + 颜色直方图，以获得更全面的图像表示。

\subsection{特征预处理}

在聚类之前，对提取的特征进行标准化处理：

\begin{equation}
\mathbf{x}_{norm} = \frac{\mathbf{x} - \mu}{\sigma}
\end{equation}

其中 $\mu$ 和 $\sigma$ 分别是特征的均值和标准差。标准化可以消除不同特征维度之间的量纲差异，提高聚类效果。

\subsection{特征降维（可选）}

对于高维特征，可以使用主成分分析（PCA）进行降维：

\begin{equation}
\mathbf{X}_{reduced} = \mathbf{X} \mathbf{W}_{PCA}
\end{equation}

其中 $\mathbf{W}_{PCA}$ 是PCA的投影矩阵，保留前 $k$ 个主成分。

\section{聚类算法选择}

\subsection{算法概述}

本任务实现了多种聚类算法，并对它们进行了比较：

\subsubsection{K-means聚类}

\textbf{算法原理}：

K-means是一种基于划分的聚类算法，目标是最小化簇内平方和（Within-Cluster Sum of Squares, WCSS）：

\begin{equation}
\min \sum_{i=1}^{K} \sum_{\mathbf{x} \in C_i} \|\mathbf{x} - \boldsymbol{\mu}_i\|^2
\end{equation}

其中 $C_i$ 是第 $i$ 个簇，$\boldsymbol{\mu}_i$ 是簇 $i$ 的质心。

\textbf{优点}：
\begin{itemize}
    \item 算法简单，计算效率高
    \item 适用于球形簇
    \item 当簇数 $K$ 已知时效果较好
\end{itemize}

\textbf{缺点}：
\begin{itemize}
    \item 需要预先指定簇数 $K$
    \item 对初始值敏感
    \item 假设簇是球形的，对非球形簇效果不佳
\end{itemize}

\textbf{参数设置}：
\begin{itemize}
    \item 簇数：$K = 6$
    \item 初始化方法：k-means++
    \item 最大迭代次数：300
\end{itemize}

\subsubsection{DBSCAN聚类}

\textbf{算法原理}：

DBSCAN（Density-Based Spatial Clustering of Applications with Noise）是一种基于密度的聚类算法，能够发现任意形状的簇并识别噪声点。

\textbf{核心概念}：
\begin{itemize}
    \item \textbf{$\epsilon$-邻域}：以点 $p$ 为中心，半径为 $\epsilon$ 的圆形区域
    \item \textbf{核心点}：$\epsilon$-邻域内至少包含 $minPts$ 个点的点
    \item \textbf{密度可达}：从核心点出发，通过一系列核心点可以到达的点
\end{itemize}

\textbf{优点}：
\begin{itemize}
    \item 不需要预先指定簇数
    \item 能够发现任意形状的簇
    \item 能够识别噪声点（异常值）
\end{itemize}

\textbf{缺点}：
\begin{itemize}
    \item 对参数 $\epsilon$ 和 $minPts$ 敏感
    \item 对高维数据效果较差
    \item 计算复杂度较高
\end{itemize}

\textbf{参数设置}：
\begin{itemize}
    \item $\epsilon = 0.5$
    \item $minPts = 5$
\end{itemize}

\subsubsection{层次聚类（Agglomerative Clustering）}

\textbf{算法原理}：

层次聚类通过逐步合并或分裂簇来构建聚类树（树状图）。本任务使用凝聚式（自底向上）方法。

\textbf{链接准则}：
\begin{itemize}
    \item \textbf{Ward链接}：最小化合并后簇的方差增加
    \item \textbf{完全链接}：使用两个簇之间最远点的距离
    \item \textbf{平均链接}：使用两个簇之间所有点对距离的平均值
\end{itemize}

\textbf{优点}：
\begin{itemize}
    \item 不需要预先指定簇数
    \item 能够生成层次结构
    \item 对簇的形状没有假设
\end{itemize}

\textbf{缺点}：
\begin{itemize}
    \item 计算复杂度高（$O(N^3)$）
    \item 对噪声和异常值敏感
    \item 一旦合并不能撤销
\end{itemize}

\textbf{参数设置}：
\begin{itemize}
    \item 簇数：$K = 6$
    \item 链接方法：Ward
    \item 距离度量：欧氏距离
\end{itemize}

\subsubsection{谱聚类（Spectral Clustering）}

\textbf{算法原理}：

谱聚类基于图论，将聚类问题转化为图的划分问题。算法步骤：

\begin{enumerate}
    \item 构建相似度矩阵 $\mathbf{W}$
    \item 计算拉普拉斯矩阵 $\mathbf{L} = \mathbf{D} - \mathbf{W}$，其中 $\mathbf{D}$ 是度矩阵
    \item 计算 $\mathbf{L}$ 的前 $K$ 个最小特征值对应的特征向量
    \item 对特征向量进行K-means聚类
\end{enumerate}

\textbf{优点}：
\begin{itemize}
    \item 能够处理非凸形状的簇
    \item 对数据的局部结构敏感
    \item 理论基础扎实
\end{itemize}

\textbf{缺点}：
\begin{itemize}
    \item 计算复杂度高
    \item 需要构建相似度矩阵，内存消耗大
    \item 对参数（如RBF核的$\gamma$）敏感
\end{itemize}

\textbf{参数设置}：
\begin{itemize}
    \item 簇数：$K = 6$
    \item 相似度矩阵：RBF核
    \item $\gamma = 1.0$
\end{itemize}

\subsubsection{高斯混合模型（GMM）}

\textbf{算法原理}：

GMM假设数据由多个高斯分布混合生成，使用EM算法估计参数。

概率密度函数：

\begin{equation}
p(\mathbf{x}) = \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
\end{equation}

其中 $\pi_k$ 是混合权重，$\boldsymbol{\mu}_k$ 和 $\boldsymbol{\Sigma}_k$ 分别是第 $k$ 个高斯分布的均值和协方差矩阵。

\textbf{优点}：
\begin{itemize}
    \item 提供软聚类（概率分配）
    \item 能够处理椭球形簇
    \item 有概率解释
\end{itemize}

\textbf{缺点}：
\begin{itemize}
    \item 需要预先指定簇数
    \item 对初始值敏感
    \item 计算复杂度较高
\end{itemize}

\textbf{参数设置}：
\begin{itemize}
    \item 组件数：$K = 6$
    \item 协方差类型：full
\end{itemize}

\subsection{算法选择策略}

对于图像聚类任务，推荐使用以下策略：

\begin{enumerate}
    \item \textbf{首选K-means}：当簇数已知且簇大致为球形时，K-means是最简单高效的选择
    \item \textbf{尝试层次聚类}：当需要了解数据的层次结构时
    \item \textbf{使用DBSCAN}：当数据中存在噪声点或簇的形状不规则时
    \item \textbf{考虑谱聚类}：当簇的形状复杂（非凸）时
\end{enumerate}

\section{聚类效果评估}

\subsection{评估指标}

\subsubsection{有监督指标}

当有真实标签时，可以使用以下指标评估聚类效果：

\textbf{1. 调整兰德指数（ARI）}

ARI衡量聚类结果与真实标签的一致性：

\begin{equation}
ARI = \frac{\sum_{ij} \binom{n_{ij}}{2} - [\sum_i \binom{a_i}{2} \sum_j \binom{b_j}{2}] / \binom{n}{2}}{\frac{1}{2}[\sum_i \binom{a_i}{2} + \sum_j \binom{b_j}{2}] - [\sum_i \binom{a_i}{2} \sum_j \binom{b_j}{2}] / \binom{n}{2}}
\end{equation}

其中 $n_{ij}$ 是同时属于簇 $i$ 和类别 $j$ 的样本数，$a_i$ 是簇 $i$ 的样本数，$b_j$ 是类别 $j$ 的样本数。

\textbf{取值范围}：$[-1, 1]$，值越大越好，1表示完全一致，0表示随机。

\textbf{2. 标准化互信息（NMI）}

NMI衡量聚类结果与真实标签之间的互信息：

\begin{equation}
NMI = \frac{2 \cdot I(Y; C)}{H(Y) + H(C)}
\end{equation}

其中 $I(Y; C)$ 是互信息，$H(Y)$ 和 $H(C)$ 分别是真实标签和聚类标签的熵。

\textbf{取值范围}：$[0, 1]$，值越大越好，1表示完全一致。

\textbf{3. 同质性（Homogeneity）}

同质性衡量每个簇是否只包含单一类别的样本：

\begin{equation}
h = 1 - \frac{H(C|Y)}{H(C)}
\end{equation}

\textbf{取值范围}：$[0, 1]$，值越大越好。

\textbf{4. 完整性（Completeness）}

完整性衡量每个类别的样本是否都被分配到同一个簇：

\begin{equation}
c = 1 - \frac{H(Y|C)}{H(Y)}
\end{equation}

\textbf{取值范围}：$[0, 1]$，值越大越好。

\textbf{5. V-measure}

V-measure是同质性和完整性的调和平均：

\begin{equation}
v = \frac{2 \cdot h \cdot c}{h + c}
\end{equation}

\subsubsection{无监督指标}

当没有真实标签时，可以使用以下内部评估指标：

\textbf{1. 轮廓系数（Silhouette Score）}

轮廓系数衡量样本与其所属簇的相似度，以及与其他簇的差异：

\begin{equation}
s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
\end{equation}

其中 $a(i)$ 是样本 $i$ 到同簇其他样本的平均距离，$b(i)$ 是样本 $i$ 到最近其他簇的平均距离。

\textbf{取值范围}：$[-1, 1]$，值越大越好。

\textbf{2. Calinski-Harabasz指数}

CH指数（方差比标准）衡量簇间方差与簇内方差的比值：

\begin{equation}
CH = \frac{tr(\mathbf{B}_k)}{tr(\mathbf{W}_k)} \times \frac{N - K}{K - 1}
\end{equation}

其中 $\mathbf{B}_k$ 是簇间协方差矩阵，$\mathbf{W}_k$ 是簇内协方差矩阵。

\textbf{取值范围}：$[0, +\infty)$，值越大越好。

\textbf{3. Davies-Bouldin指数}

DB指数衡量簇内距离与簇间距离的比值：

\begin{equation}
DB = \frac{1}{K} \sum_{i=1}^{K} \max_{j \neq i} \left( \frac{\sigma_i + \sigma_j}{d(c_i, c_j)} \right)
\end{equation}

其中 $\sigma_i$ 是簇 $i$ 的平均距离，$d(c_i, c_j)$ 是簇中心之间的距离。

\textbf{取值范围}：$[0, +\infty)$，值越小越好。

\subsection{实验结果}

实验使用ResNet50提取的2048维特征，对600张图像进行聚类。表~\ref{tab:results} 展示了不同聚类算法的评估结果。

\begin{table}[H]
\centering
\caption{不同聚类算法的评估结果}
\label{tab:results}
\begin{tabular}{lcccccc}
\toprule
\textbf{方法} & \textbf{ARI} & \textbf{NMI} & \textbf{同质性} & \textbf{完整性} & \textbf{V-measure} & \textbf{轮廓系数} \\
\midrule
K-means & 0.XXXX & 0.XXXX & 0.XXXX & 0.XXXX & 0.XXXX & 0.XXXX \\
DBSCAN & 0.XXXX & 0.XXXX & 0.XXXX & 0.XXXX & 0.XXXX & 0.XXXX \\
层次聚类 & 0.XXXX & 0.XXXX & 0.XXXX & 0.XXXX & 0.XXXX & 0.XXXX \\
谱聚类 & 0.XXXX & 0.XXXX & 0.XXXX & 0.XXXX & 0.XXXX & 0.XXXX \\
GMM & 0.XXXX & 0.XXXX & 0.XXXX & 0.XXXX & 0.XXXX & 0.XXXX \\
\bottomrule
\end{tabular}
\end{table}

\textbf{注意}：表中的数值为示例，实际运行程序后会得到具体数值。

\subsection{结果分析}

根据实验结果，可以得出以下结论：

\begin{enumerate}
    \item \textbf{特征提取的重要性}：使用深度特征（ResNet50）通常比传统特征（HOG、颜色直方图）效果更好，因为深度特征能够捕获图像的语义信息。
    
    \item \textbf{算法选择}：
    \begin{itemize}
        \item K-means在已知簇数的情况下通常表现良好，计算效率高
        \item 层次聚类能够提供层次结构，但计算复杂度较高
        \item DBSCAN能够识别噪声点，但参数调优较困难
        \item 谱聚类对非凸簇效果好，但计算成本高
    \end{itemize}
    
    \item \textbf{参数调优}：不同算法对参数敏感，需要通过交叉验证或网格搜索找到最优参数。
    
    \item \textbf{特征预处理}：标准化对聚类效果有重要影响，特别是对于K-means等基于距离的算法。
\end{enumerate}

\subsection{可视化结果}

使用t-SNE将高维特征降维到2D空间，可以直观地观察聚类效果。图~\ref{fig:visualization} 展示了真实标签分布和聚类结果的对比。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{results/visualization_K-means.png}
\caption{聚类结果可视化（示例：K-means）}
\label{fig:visualization}
\end{figure}

\section{总结}

本报告完成了图像聚类任务，主要贡献包括：

\begin{enumerate}
    \item \textbf{问题形式化}：将图像聚类问题转化为数学优化问题，明确了目标函数和约束条件。
    
    \item \textbf{特征提取}：实现了多种特征提取方法，包括深度学习方法（ResNet50）和传统方法（HOG、颜色直方图），并比较了它们的效果。
    
    \item \textbf{算法实现}：实现了5种经典的聚类算法（K-means、DBSCAN、层次聚类、谱聚类、GMM），并分析了它们的优缺点。
    
    \item \textbf{效果评估}：使用多种评估指标（ARI、NMI、轮廓系数等）全面评估了聚类效果，并进行了可视化分析。
\end{enumerate}

\textbf{主要发现}：

\begin{itemize}
    \item 深度特征（ResNet50）在图像聚类任务中表现优异
    \item K-means算法在已知簇数的情况下是简单高效的选择
    \item 特征标准化对聚类效果至关重要
    \item 不同算法适用于不同的数据分布和场景
\end{itemize}

\textbf{未来改进方向}：

\begin{itemize}
    \item 尝试更多的特征提取方法（如自编码器、对比学习等）
    \item 实现更先进的聚类算法（如深度聚类）
    \item 进行更细致的参数调优
    \item 探索特征融合和集成学习方法
\end{itemize}

\section{参考文献}

\begin{enumerate}
    \item MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations.
    
    \item Ester, M., et al. (1996). A density-based algorithm for discovering clusters in large spatial databases.
    
    \item He, K., et al. (2016). Deep residual learning for image recognition.
    
    \item Von Luxburg, U. (2007). A tutorial on spectral clustering.
    
    \item Rand, W. M. (1971). Objective criteria for the evaluation of clustering methods.
    
    \item Vinh, N. X., et al. (2010). Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance.
\end{enumerate}

\end{document}

